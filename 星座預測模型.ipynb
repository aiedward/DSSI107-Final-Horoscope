{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def readfile(filename,encode = 'UTF-8'):\n",
    "    rawlist = []\n",
    "    with open(filename,'r+', encoding=encode) as csvfile:\n",
    "        rows = csv.reader(csvfile)\n",
    "        for row in rows:\n",
    "            rawlist.append(row)\n",
    "    return rawlist\n",
    "\n",
    "def writefile(filename,inputlist):\n",
    "    with open(filename,'a+', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',')\n",
    "        for row in inputlist:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 唐跟蘇珊兩邊的碼拌在一起，有把需要的步驟函數化，主要看函數即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tang = readfile('tang_content.csv')\n",
    "susan = readfile('susan.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 做字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "\n",
    "for i in range(0,len(tang)):\n",
    "    for j in range(1,len(tang[i])):\n",
    "        for char in tang[i][j]:\n",
    "            if char not in word_dict:\n",
    "                word_dict[char] = 1\n",
    "            else:\n",
    "                word_dict[char] = word_dict[char]+1\n",
    "\n",
    "for i in range(0,len(susan)):\n",
    "    for j in range(1,len(susan[i])):\n",
    "        for char in susan[i][j]:\n",
    "            if char not in word_dict:\n",
    "                word_dict[char] = 1\n",
    "            else:\n",
    "                word_dict[char] = word_dict[char]+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 把字典排序，並拿掉出現頻率太低的字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_word_dict = [(k, word_dict[k]) \n",
    "                    for k in sorted(word_dict, key=word_dict.get, reverse=True)]\n",
    "too_high = 400 \n",
    "too_low = 20\n",
    "\n",
    "final_dict = []\n",
    "\n",
    "for i in range(0,len(sorted_word_dict)):\n",
    "    if not sorted_word_dict[i][1] < too_low and not sorted_word_dict[i][1] > too_high:\n",
    "        final_dict.append(sorted_word_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1038"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 把每一篇文章變成陣列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(inputlist):\n",
    "    inputsum = sum(inputlist)\n",
    "    if not inputsum==0:\n",
    "        for i in range(0,len(inputlist)):\n",
    "            if not (inputlist[i]/inputsum) == None:\n",
    "                inputlist[i] = inputlist[i]/inputsum\n",
    "            else:\n",
    "                print('Warning: something weird happened while dividing')\n",
    "    else:\n",
    "        print('Warning: input sum==0')\n",
    "    return inputlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def articletoVector(article_set,final_dict,cuthead=1):\n",
    "    variable_bracket = np.zeros(len(final_dict))\n",
    "    output_vector = []\n",
    "    tmp = []\n",
    "\n",
    "    for i in range(0,len(article_set)):\n",
    "        tmp.append(article_set[i][0])\n",
    "        for j in range(cuthead,len(article_set[i])):\n",
    "            for char in article_set[i][j]:\n",
    "                for pos in range(0,len(final_dict)):\n",
    "                    if char == final_dict[pos][0]:\n",
    "                        variable_bracket[pos] = variable_bracket[pos] + 1/final_dict[pos][1]\n",
    "                        break\n",
    "            variable_bracket = normalize(variable_bracket)\n",
    "            tmp.append(variable_bracket)    \n",
    "            variable_bracket = np.zeros(len(final_dict))\n",
    "        output_vector.append(tmp)\n",
    "        tmp = [] \n",
    "    return(output_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "susan_vector = articletoVector(susan,final_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_bracket = np.zeros(len(final_dict))\n",
    "tang_vector = []\n",
    "tmp = []\n",
    "\n",
    "for i in range(0,len(tang)):\n",
    "    tmp.append(tang[i][0])\n",
    "    for j in range(1,len(tang[i])):\n",
    "        #print(len(tang[i][j]))\n",
    "        for char in tang[i][j]:\n",
    "            for pos in range(0,len(final_dict)):\n",
    "                if char == final_dict[pos][0]:\n",
    "                    variable_bracket[pos] = variable_bracket[pos] + 1/final_dict[pos][1]\n",
    "                    break\n",
    "        variable_bracket = normalize(variable_bracket)\n",
    "        tmp.append(variable_bracket)    \n",
    "        variable_bracket = np.zeros(len(final_dict))\n",
    "    tang_vector.append(tmp)\n",
    "    tmp = []  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 引入每篇文章的分數，並結合tang_vector產生train_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = [0,5,5,5,4,4,4,3,3,2,2,1,1]\n",
    "train_source = []\n",
    "tmp = []\n",
    "for i in range(0,len(tang_vector)):\n",
    "    for pos in range(1,len(tang_vector[i])):\n",
    "        tmp.append(score[pos])\n",
    "        for item in tang_vector[i][pos]:\n",
    "            tmp.append(item)\n",
    "        train_source.append(tmp)\n",
    "        tmp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_data(raw_vector,score=[0],with_score=0,cuthead=1):\n",
    "    output = []\n",
    "    tmp = []\n",
    "    for i in range(0,len(raw_vector)):\n",
    "        for pos in range(cuthead,len(raw_vector[i])):\n",
    "            if with_score==1:\n",
    "                tmp.append(score[pos])\n",
    "            for item in raw_vector[i][pos]:\n",
    "                tmp.append(item)\n",
    "            output.append(tmp)\n",
    "            tmp = []\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 準備測試資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700\n",
      "1063\n"
     ]
    }
   ],
   "source": [
    "X = train_source\n",
    "random.shuffle(X)\n",
    "Y = []\n",
    "for i in range(0,len(X)):\n",
    "    Y.append(train_source[i][0])\n",
    "    del X[i][0]\n",
    "\n",
    "# Split the data into training/testing sets\n",
    "X_train = X[:2700]\n",
    "X_test = X[2700:]\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "Y_train = Y[:2700]\n",
    "Y_test = Y[2700:]\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression 表現次佳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create linear regression object\n",
    "regr = linear_model.LogisticRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.3828793 0.7102543\n"
     ]
    }
   ],
   "source": [
    "correct=0\n",
    "soso=0\n",
    "\n",
    "for i in range(0,len(Y_test)):\n",
    "    if abs(Y_pred[i]-Y_test[i])==0:\n",
    "        correct = correct+1\n",
    "    elif abs(Y_pred[i]-Y_test[i])==1:\n",
    "        soso=soso+1\n",
    "print('Logistic Regression: %f3'%(correct/len(Y_test)),'%f3'%((soso+correct)/len(Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine 表現在標準化後不太好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(gamma='scale')\n",
    "clf.fit(X_train, Y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: 0.2314213 0.6688623\n"
     ]
    }
   ],
   "source": [
    "correct=0\n",
    "soso=0\n",
    "\n",
    "for i in range(0,len(Y_test)):\n",
    "    if abs(Y_pred[i]-Y_test[i])==0:\n",
    "        correct = correct+1\n",
    "    elif abs(Y_pred[i]-Y_test[i])==1:\n",
    "        soso=soso+1\n",
    "print('SVM: %f3'%(correct/len(Y_test)),'%f3'%((soso+correct)/len(Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest 表現最佳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=None,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "Rforest = ensemble.RandomForestClassifier(n_estimators=1000, random_state=0)  \n",
    "Rforest.fit(X_train, Y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = Rforest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: 0.4261523 0.7676393\n"
     ]
    }
   ],
   "source": [
    "correct=0\n",
    "soso=0\n",
    "\n",
    "for i in range(0,len(Y_test)):\n",
    "    if abs(Y_pred[i]-Y_test[i])==0:\n",
    "        correct = correct+1\n",
    "    elif abs(Y_pred[i]-Y_test[i])==1:\n",
    "        soso=soso+1\n",
    "print('Random Forest: %f3'%(correct/len(Y_test)),'%f3'%((soso+correct)/len(Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用唐的模型拿來算蘇珊的分數，很怪的方法，下次不敢了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_susan = create_train_data(susan_vector)\n",
    "susan_predict = classifier.predict(X_susan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "susan_predict_output = []\n",
    "tmp = []\n",
    "tot = 0\n",
    "for i in range(len(susan)):\n",
    "    tmp.append(susan[i][0])\n",
    "    for j in range(0,12):\n",
    "        tmp.append(susan_predict[tot+j])\n",
    "    tot = tot+12\n",
    "    susan_predict_output.append(tmp)\n",
    "    tmp = []\n",
    "writefile('susan_daily_score.csv',susan_predict_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 蘇珊周運勢的計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: input sum==0\n"
     ]
    }
   ],
   "source": [
    "susan_week = readfile('susan_week.csv')\n",
    "susan_week_raw = []\n",
    "for i in range(0,len(susan_week)):\n",
    "    susan_week_raw.append([susan_week[i][0]])\n",
    "    \n",
    "susan_week_vector = articletoVector(susan_week_raw,final_dict,cuthead=0)\n",
    "susan_week_X = create_train_data(susan_week_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 4 4 4 3 4 4 3 5 2 1 4 5 1 2 4 4 2 4 5 4 3 3 5 4 1 3 5 2 4 1 1 4 1 4 4 3\n",
      " 1 3 1 4 4 1 3 4 5 5 1 4 4 1 1 5 4 5 4 4 4 2 4 2 5 2 2 1 4 5 2 5 4 1 1 4 1\n",
      " 3 4 4 4 1 1 4 5 4 4 4 1 1 2 5 4 5 1 4 1 1 5 4 4 1 1 5 1 4 2 4 1 3 3 3 4 1\n",
      " 5 3 4 4 1 1 5 4 3 4 1 5 1 5 4 5 5 3 3 1 3 4 5 2 1 4 4 1 1 3 5 1 1 4 1 4 1\n",
      " 4 1 1 1 4 2 4 1]\n"
     ]
    }
   ],
   "source": [
    "Y_pred = classifier.predict(susan_week_X)\n",
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(susan_week)):\n",
    "    susan_week[i].append(Y_pred[i])\n",
    "writefile('susan_week_with_value.csv',susan_week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 唐周運勢的計算，因為文字量太少，有算跟沒算一樣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "tang_week = readfile('tang_week.csv')\n",
    "tang_week_raw = []\n",
    "for i in range(0,len(tang_week)):\n",
    "    tang_week_raw.append([tang_week[i][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: input sum==0\n",
      "Warning: input sum==0\n",
      "Warning: input sum==0\n",
      "Warning: input sum==0\n",
      "Warning: input sum==0\n",
      "Warning: input sum==0\n",
      "[1 5 1 5 1 3 5 5 1 1 4 2 1 4 2 1 1 1 1 1 4 4 5 1 1 1 3 4 5 4 1 2 5 5 4 4 2\n",
      " 1 5 1 1 5 5 5 2 5 4 4 1 1 2 4 5 1 4 1 1 4 5 4 4 1 4 3 4 1 3 1 5 1 1 4 1 4\n",
      " 4 1 4 5 5 1 5 1 2 5 1 1 4 4 5 2 5 1 2 1 4 3 1 1 1 1 5 2 1 1 4 5 4 1 4 1 2\n",
      " 5 1 3 1 4 4 4 4 1 4 1 3 4 1 5 1 4 4 5 4 5 2 1 1 4 5 1 1 5 5 5 5 1 4 1 5 1\n",
      " 2 1 5 5 5 1 4 1]\n"
     ]
    }
   ],
   "source": [
    "tang_week_vector = articletoVector(tang_week_raw,final_dict,cuthead=0)\n",
    "tang_week_X = create_train_data(tang_week_vector)\n",
    "Y_pred = classifier.predict(tang_week_X)\n",
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(tang_week)):\n",
    "    tang_week[i].append(Y_pred[i])\n",
    "writefile('tang_week_with_value.csv',tang_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
